{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Perceptron\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On initialise les paramètres du perceptron\n",
    "def initParam(n):\n",
    "    W=np.random.randn(n,1)\n",
    "    b=np.random.randn(1)\n",
    "    return (W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction générique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit notre modèle avec la fonction sigmoïde\n",
    "def perceptron(X,W,b):\n",
    "    \n",
    "    z = (X.dot(W)+ b)\n",
    "    A = 1 / (1+np.exp(-z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erreur quadratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ErreurQ\n",
    "def errQ(A,y):\n",
    "    m=len(y)\n",
    "    return np.sum((A-y)**2)/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rapport avec la gradient\n",
    "# On détermine le gradient de l'erreur quadratique\n",
    "def gradient_erreurQ(A,X,y):\n",
    "    \n",
    "    m = len(y)\n",
    "    db = np.sum( (A-y) * A * (1-A) ) /m\n",
    "    dW = np.dot(X.T,(A-y) * A * (1-A) ) /m\n",
    "    \n",
    "    return (dW,db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rapport avec le log_loss\n",
    "def logloss(A,y):\n",
    "    eps=1e-15\n",
    "    m=len(y)\n",
    "    return -(np.sum(y*np.log(A+eps)+(1-y)*np.log(1-A+eps)))/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_log_loss(A,X,y):\n",
    "    \n",
    "    m = len(y)\n",
    "    db = np.sum( (A-y)) /m\n",
    "    dW = np.dot(X.T,(A-y)) /m\n",
    "    \n",
    "    return (dW,db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on met à jour les paramètres par descente de gradient\n",
    "def mAj(dW,db,W,b,alpha):\n",
    "    \n",
    "    b = b - alpha * db\n",
    "    w = W - alpha * dW \n",
    "    \n",
    "    return (w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X les données en entrées\n",
    "# selecteur permet de choisir entre : -log_loss (maximisation de la vraissemblance) -erreur_quadratique (minimisation de l'erreur quadratique)\n",
    "# y les résultats associé aux données\n",
    "def module_perceptron(W,b , X,y,selecteur,alpha=0.1, iter=100):\n",
    "    if selecteur == 'log_loss':\n",
    "        eps=1e-15\n",
    "        m=len(y)\n",
    "        for i in range(iter):\n",
    "            #definition du perceptron\n",
    "            A = perceptron(X,W,b)\n",
    "            '''\n",
    "            #calcul de la moyenne d'echec\n",
    "            if i%2==0:\n",
    "                perf = -(np.sum(y*np.log(A+eps)+(1-y)*np.log(1-A+eps)))/m\n",
    "            '''\n",
    "            #calcul du gradient\n",
    "            db = np.sum( (A-y)) /m\n",
    "            dW = np.dot(X.T,(A-y)) /m\n",
    "            \n",
    "            #mise a jour de l'erreur\n",
    "            b = b - alpha * db\n",
    "            W = W - alpha * dW \n",
    "        return W,b\n",
    "    elif selecteur == 'erreur_quadratique':\n",
    "        m=len(y)\n",
    "        for i in range(iter):\n",
    "            #definition du perceptron\n",
    "            A = perceptron(X,W,b)\n",
    "            '''\n",
    "            #calcul de la moyenne d'echec\n",
    "            if i%2==0:\n",
    "                perf = np.sum((A-y)**2)/m\n",
    "            '''\n",
    "            #calcul du gradient\n",
    "            db = np.sum( (A-y) * A * (1-A) ) /m\n",
    "            dW = np.dot(X.T,(A-y) * A * (1-A) ) /m\n",
    "            \n",
    "            #mise a jour de l'erreur\n",
    "            b = b - alpha * db\n",
    "            w = W - alpha * dW \n",
    "        return W,b\n",
    "    else:\n",
    "        print(\"Unknow parameter\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image,W,b):\n",
    "    return (perceptron(image,W,b) > 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fonctions d'affichage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3626134271.py, line 76)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [10]\u001b[0;36m\u001b[0m\n\u001b[0;31m    c=\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def affichage_iter(nb,W,b,selecteur,X_train,y_train,X_test,y_test):\n",
    "    if nb < 100:\n",
    "        print(\"Il faut augmenter le nombre d'iteration - 100 est le minimum\")\n",
    "        raise\n",
    "        \n",
    "    \n",
    "    alpha = 0.1\n",
    "    eps=1e-15\n",
    "    m=len(y_train)\n",
    "    mt=len(y_test)\n",
    "    \n",
    "    nb_point = 10\n",
    "    \n",
    "    #################### LOG LOSS\n",
    "    if selecteur == 'log_loss':    \n",
    "        acc_ll = []\n",
    "        acc_llt = []\n",
    "        cout_ll = []\n",
    "        cout_llt = []\n",
    "\n",
    "        print(\"MAXIMISATION DE LA VRAISSEMBLANCE\")\n",
    "        for i in range(100,nb):\n",
    "            #definition du perceptron\n",
    "            A = perceptron(X_train,W,b)\n",
    "\n",
    "            #calcul de la moyenne d'echec\n",
    "            if i%nb_point==0:\n",
    "                c = -(np.sum(y_train*np.log(A+eps)+(1-y_train)*np.log(1-A+eps)))/m\n",
    "                cout_ll.append(c)\n",
    "                A_test = perceptron(X_test,W,b)\n",
    "                ct =  -(np.sum(y_test*np.log(A_test+eps)+(1-y_test)*np.log(1-A_test+eps)))/mt\n",
    "                cout_llt.append(ct)\n",
    "\n",
    "                a = accuracy_score(y_train,predict(X_train,W,b))\n",
    "                acc_ll.append(a)\n",
    "                at = accuracy_score(y_test,predict(X_test,W,b))\n",
    "                acc_llt.append(at)\n",
    "                pass\n",
    "            \n",
    "            #calcul du gradient\n",
    "            db = np.sum( (A-y_train)) /m\n",
    "            dW = np.dot(X_train.T,(A-y_train)) /mt\n",
    "\n",
    "            #mise a jour de l'erreur\n",
    "            b = b - alpha * db\n",
    "            W = W - alpha * dW\n",
    "            pass\n",
    "\n",
    "        #affichage des resultats\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot (cout_ll,label='Données d''entraînement')\n",
    "        plt.plot (cout_llt,label='Données de test')\n",
    "        plt.legend()\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(acc_ll,label='Données d''entraînement')\n",
    "        plt.plot(acc_llt,label='Données de test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    #################### ERREUR QUADRATIQUE \n",
    "    elif selecteur == 'erreur_quadratique':\n",
    "        acc_eq = []\n",
    "        acc_eqt = []\n",
    "        cout_eq = []\n",
    "        cout_eqt = []\n",
    "        print(\"REDUCTION DE L'ERREUR QUADRATIQUE\")\n",
    "\n",
    "        for i in range(100,nb):\n",
    "            #definition du perceptron\n",
    "            A = perceptron(X_train,W,b)\n",
    "\n",
    "            #calcul de la moyenne d'echec\n",
    "            if i%nb_point==0:\n",
    "                c = np.sum((A-y_train)**2)/m\n",
    "                cout_eq.append(c) \n",
    "                A_test = perceptron(X_test,W,b)\n",
    "                ct = np.sum((A_test-y_test)**2)/mt\n",
    "                cout_eqt.append(ct)\n",
    "\n",
    "                a = accuracy_score(y_train,predict(X_train,W,b))\n",
    "                acc_eq.append(a)\n",
    "                at = accuracy_score(y_test,predict(X_test,W,b))\n",
    "                acc_eqt.append(at)\n",
    "                pass\n",
    "            \n",
    "            #calcul du gradient\n",
    "            db = np.sum( (A-y_train) * A * (1-A) ) /m\n",
    "            dW = np.dot(X_train.T,(A-y_train) * A * (1-A) ) /m\n",
    "\n",
    "            #mise a jour de l'erreur\n",
    "            b = b - alpha * db\n",
    "            w = W - alpha * dW \n",
    "            pass\n",
    "    \n",
    "        #affichage des resultats\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.subplot(1,2,1)\n",
    "        plt.plot (cout_eq,label='Données d''entraînement')\n",
    "        plt.plot (cout_eqt,label='Données de test')\n",
    "        plt.legend()\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(acc_eq,label='Données d''entraînement')\n",
    "        plt.plot(acc_eqt,label='Données de test')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        print(\"Unknow parameter\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
