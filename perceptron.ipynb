{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Perceptron\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On initialise les paramètres du perceptron\n",
    "def initParam(n):\n",
    "    W=np.random.randn(n,1)\n",
    "    b=np.random.randn(1)\n",
    "    return (W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction générique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit notre modèle avec la fonction sigmoïde\n",
    "def perceptron(X,W,b):\n",
    "    \n",
    "    z = (X.dot(W)+ b)\n",
    "    A = 1 / (1+np.exp(-z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Erreur quadratique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ErreurQ\n",
    "def errQ(A,y):\n",
    "    m=len(y)\n",
    "    return np.sum((A-y)**2)/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rapport avec la gradient\n",
    "# On détermine le gradient de l'erreur quadratique\n",
    "def gradient_erreurQ(A,X,y):\n",
    "    \n",
    "    m = len(y)\n",
    "    db = np.sum( (A-y) * A * (1-A) ) /m\n",
    "    dW = np.dot(X.T,(A-y) * A * (1-A) ) /m\n",
    "    \n",
    "    return (dW,db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rapport avec le log_loss\n",
    "def logloss(A,y):\n",
    "    eps=1e-15\n",
    "    m=len(y)\n",
    "    return -(np.sum(y*np.log(A+eps)+(1-y)*np.log(1-A+eps)))/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_log_loss(A,X,y):\n",
    "    \n",
    "    m = len(y)\n",
    "    db = np.sum( (A-y)) /m\n",
    "    dW = np.dot(X.T,(A-y)) /m\n",
    "    \n",
    "    return (dW,db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descente de gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on met à jour les paramètres par descente de gradient\n",
    "def mAj(dW,db,W,b,alpha):\n",
    "    \n",
    "    b = b - alpha * db\n",
    "    w = W - alpha * dW \n",
    "    \n",
    "    return (w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X les données en entrées\n",
    "# selecteur permet de choisir entre : -log_loss (maximisation de la vraissemblance) -erreur_quadratique (minimisation de l'erreur quadratique)\n",
    "# y les résultats associé aux données\n",
    "def module_perceptron(X,selecteur,y):\n",
    "    if selecteur == 'log_loss':\n",
    "        # init\n",
    "        (W,b) = initParam(X.shape[1])\n",
    "        for i in range(iter):\n",
    "            #definition du perceptron\n",
    "            A = perceptron(X,W,b)\n",
    "\n",
    "            #calcul de la moyenne d'echec\n",
    "            if i%3==0:\n",
    "                perf = log_loss(A,y)\n",
    "\n",
    "            #calcul du gradient\n",
    "            (dW,db) = gradient_log_loss(A,X,y)\n",
    "            #mise a jour de l'erreur\n",
    "            W,b = mAj(dW,db,W,b,alpha)\n",
    "    elif selecteur == 'erreur_quadratique':\n",
    "        # init\n",
    "        (W,b) = initParam(X.shape[1])\n",
    "        for i in range(iter):\n",
    "            #definition du perceptron\n",
    "            A = perceptron(X,W,b)\n",
    "\n",
    "            #calcul de la moyenne d'echec\n",
    "            if i%3==0:\n",
    "                perf = errQ(A,y)\n",
    "\n",
    "            #calcul du gradient\n",
    "            (dW,db) = gradient_erreurQ(A,X,y)\n",
    "            #mise a jour de l'erreur\n",
    "            W,b = mAj(dW,db,W,b,alpha)\n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonction de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image,W,b):\n",
    "    return perceptron(image,W,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def succes(liste_images,y,W,b):\n",
    "    nb_succes = 0\n",
    "    N = len(liste_images)\n",
    "    for i in range(N):\n",
    "        if perceptron(liste_images[i],W,b) == y[i]:\n",
    "            nb_succes += 1\n",
    "    return nb_succes/N"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
